{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conventional neural networks are not well designed to model the uncertainty associated with the predictions they make. For that, one way is to go full Bayesian. \n",
    "\n",
    "In this class we will introduce the concept of Bayesian Neural Networks (BNN) by studing a standard supervised use case: a toy regression problem. For that, we will first implement linear regression and learn point estimates for the parameters w and b. Then we’ll see how to incorporate uncertainty into our estimates by using MCMC to implement Bayesian linear regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Bayesian neural network is a neural network with a prior distribution on its weights. This means that, in contrast with any convential (non-Bayesian) neural network, BNNs are interested not only in the optimal values of the network's parameters -- weights and biases -- but also in the distribution associated with them. Thanks to these distributions we could have a certain level of confidence about the network predictions.\n",
    "\n",
    "The  idea  behind  Bayesian  neural  networks is then to cast the task of training a network as a problem of inference, which is solved using Bayes’ theorem. The latter theorem is used to assign a probability density to each point in the parameter space of the neural network, as it is featured below. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes Formula\n",
    "\n",
    "Estimating the distributions associated with the network parameters is hard. These are generally referred to as posterior densities, and are estimated using the Bayes rule.\n",
    "\n",
    "$$p \\left(w \\mid x,y \\right) = \\frac{p \\left( x,y \\mid w \\right) p(w)}{\\int p \\left( x, y \\mid w\\right) p(w)dw}$$\n",
    "\n",
    "The main problem lies in the denominator — also known as model evidence. It requires integrating over all possible values of the parameters (i.e., all weights and biases space), and it is often not doable in practice.\n",
    "\n",
    "Instead, pseudo-numerical approaches can be chosen where the solution to those integrals is approximated. The most common approaches are: \n",
    "\n",
    "1.- Approximating the integral with MCMC\n",
    "\n",
    "2.- Using variational inference \n",
    "\n",
    "3.-Using MC dropout\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In this notebook we will explore the MCMC approach with the help of the tensorflow_probability library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But first, let's dive deep into the basic principles with the regression example.\n",
    "\n",
    "Regression is one of the most common and basic supervised learning tasks in machine learning.\n",
    "\n",
    "Considering $\\mathcal{D} = \\left( \\boldsymbol{X},\\boldsymbol{Y} \\right) = \\left\\{ (\\boldsymbol{x}_i,y_i) \\right\\}_{i=1}^{N}$. Suppose there exist $f(x)$ so that \n",
    "\n",
    "$$y = f(x)$$\n",
    "\n",
    "We want to find $f(x) = \\phi_w (x)$ where $w$ are the parameters of the later function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s first implement linear regression and learn point estimates for the parameters w and b. Then we’ll see how to incorporate uncertainty into our estimates to implement Bayesian linear regression.\n",
    "\n",
    "But, first of all, let’s import the modules we’ll need!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.layers import Input\n",
    "from keras import backend as K\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.optimizers import Adam\n",
    "from keras.engine.topology import Layer\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "# config.gpu_options.allow_growth = True\n",
    "sess = tf.InteractiveSession(config=config)\n",
    "\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a toy dataset of 100 data points with one feature and w=3.5 and b=-9.3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now add some constant noise in the predicted dimension:\n",
    "\n",
    "$$y = f(x) + \\epsilon$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, the problem state as $$y = \\phi_w (x) + \\epsilon$$\n",
    "\n",
    "$$ p(y \\mid x, w, \\epsilon) = \\mathcal{N}(y \\mid \\phi_w(x), \\epsilon^2)$$\n",
    "\n",
    "where $\\phi_w (x) = w_1 x + w_2$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ p(y_{i}\\mid x_{i},w, \\epsilon)=\\frac{1}{\\sqrt{2\\pi\\epsilon^{2}}}\\exp\\left(-\\frac{\\left(y_{i}-\\phi_{w}(x_{i})\\right)^{2}}{2\\epsilon^{2}}\\right)$$\n",
    "\n",
    "$$\\Rightarrow-\\ln\\left(p(y_{i}\\mid x_{i},w)\\right)\\Big|_{\\{w,\\epsilon\\}}=-\\ln(\\epsilon)-\\frac{\\left(y_{i}-\\phi_{w}(x_{i})\\right)^{2}}{2\\epsilon^{2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define using keras (K) the second term of the above formula:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_normal_pdf(y, output_psi, epsilon):\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom keras' layer to deal with scalars:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a function which retuns the ELU value of x + 1 given x:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import ELU\n",
    "\n",
    "def elu_plus1(x, a=1.):\n",
    "    return ELU(alpha=a)(x)+1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the predictions and visualize the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.arange(low,high,0.01).reshape(-1,1)\n",
    "y_test = f(X_test) + np.random.normal(\n",
    "    0,std,size=(X_test.shape[0])).reshape(-1,1)\n",
    "\n",
    "prediction = model.predict(X_test)\n",
    "\n",
    "plt.errorbar(X_test,prediction[:,0],\n",
    "             yerr=2*prediction[:,1],\n",
    "             color='#0A5FB4',\n",
    "             alpha=0.8,\n",
    "             label='prediction')\n",
    "\n",
    "plt.plot(X_test, y_test,'x',c='r',\n",
    "         alpha=0.8, label='real generated data')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable noise\n",
    "\n",
    "Let's now assume that our noise is not constant but a function of x:\n",
    "\n",
    "$$y = f(x) + \\epsilon(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, generate the data according to this new condition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have $$y = \\phi_w (x) + \\psi_v (x)$$\n",
    "\n",
    "$$ p(y \\mid x, w, v) = \\mathcal{N}(y \\mid \\phi_w(x), \\psi_v^2(x))=$$\n",
    "            $$= \\frac{1}{\\sqrt{2\\pi \\psi_v(x)^2}} e^{-\\frac{(y-\\phi_w (x))^2}{2\\psi_v(x)^2}}$$\n",
    "\n",
    "i.e. We estimate a variability of $y_i$ for every data point $x_i$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the new regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict and visualize the results. Have we catched the heteroscedastic uncertainty?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.arange(low,high,0.01).reshape(-1,1)\n",
    "y_test = f(X_test) + np.random.normal(\n",
    "    0,std,size=(X_test.shape[0])).reshape(-1,1)+[\n",
    "    np.random.normal(0,i) for i in (np.sin(X_test*-10))+1]\n",
    "\n",
    "prediction = model.predict(X_test)\n",
    "\n",
    "plt.errorbar(X_test,prediction[:,0],\n",
    "             yerr=2*prediction[:,1],\n",
    "             color='#0A5FB4',\n",
    "             alpha=0.8,\n",
    "             label='prediction')\n",
    "\n",
    "plt.plot(X_test, y_test,'x',c='r',\n",
    "         alpha=0.8, label='real generated data')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with the Epistemic uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside the *uncertainty quantification* field there is an approach that classifies uncertainty in two different categories:\n",
    "\n",
    "1. Aleatoric Uncertainty: If there exists a variability in our possible correct predictions for the same initial inputs (see previous section).\n",
    "\n",
    "2. Epistemic Uncertainty: related to our ignorance:\n",
    "\n",
    "     2.1. We are not using the correct model $\\phi_w$ to approximate the hyphothetical function $f$\n",
    "     \n",
    "     2.2. Our incomplete knowledge of the types of data that exists.\n",
    "\n",
    "There are different techniques in the literature for modeling the epistemic uncertainty. Here, we will cover the MCMC approach using tensorflow and the tensorflow_probability library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem**. Consider a data set $\\{(\\mathbf{x}_n, y_n)\\}$, where each data point comprises of features $\\mathbf{x}_n\\in\\mathbb{R}^D$ and output $y_n\\in\\mathbb{R}$. Define the likelihood for each data point as $$\\begin{aligned} p(y_n \\mid \\mathbf{w}, \\mathbf{x}_n, \\sigma^2) &= \\text{Normal}(y_n \\mid \\mathrm{NN}(\\mathbf{x}_n\\;;\\;\\mathbf{w}), \\sigma^2),\\end{aligned}$$\n",
    "\n",
    "where $\\mathrm{NN}$ is a neural network whose weights and biases form the latent variables $\\mathbf{w}$. Assume $\\sigma^2$ is a known variance.\n",
    "\n",
    "Define the prior on the weights and biases $\\mathbf{w}$ to be the standard normal $$\\begin{aligned} p(\\mathbf{w}) &= \\text{Normal}(\\mathbf{w} \\mid \\mathbf{0}, \\mathbf{I}).\\end{aligned}$$\n",
    "\n",
    "The question here is: how these parameters (now random variables) are distributed (~posterior distribution)? Could you give an estimation of these variables?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solving the inference problem using MCMC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we noted in the introduction of this notebook, for most practical problems of interest exact inference is hard or can not be performed analytically. That is why some form of approximation need to be done. \n",
    "\n",
    "In this section we consider approximate inference methods based of numerical sampling to get the distribution of the latent variables.\n",
    "The main idea behing MCMC is to generate samples from the posterior distribution by constructing a reversible Markov-chain that has as its equilibrium distribution the target posterior distribution. In essence, MCMC will allow us finding the expectation of some function with repect to a probability distribution -- for instance, mean and variance of the latent variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ex. Using the Hamiltonian Monte Carlo `tfp.mcmc.HamiltonianMonteCarlo` method estimate the weights and biases of a linear regression problem.\n",
    "\n",
    "Use as data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_toy_dataset(N, w, noise_std=0.1):\n",
    "    x = np.random.randn(N)\n",
    "    y = x * w + np.random.normal(0, noise_std, size=N)\n",
    "    return x, y\n",
    "\n",
    "N = 40  # number of data points\n",
    "D = 1  # number of features\n",
    "\n",
    "w_true = np.random.randn()\n",
    "X_train, y_train = build_toy_dataset(N, w_true)\n",
    "X_test, y_test = build_toy_dataset(N, w_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the linear model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow_probability import edward2 as ed\n",
    "\n",
    "#tf.reset_default_graph()\n",
    "\n",
    "def linear_model(x_data=X_train):\n",
    "    A = ed.Normal(loc=0., scale=10., name=\"A\")  \n",
    "    b = ed.Normal(loc=0., scale=10., name=\"b\")  \n",
    "    mu = A * x_data + b\n",
    "    y_data = ed.Normal(loc=mu, scale=tf.ones(N), name=\"y_data\")  # `y` above\n",
    "    return y_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the log-joint probability function using `ed.make_log_joint_fn`, and implements MCMC with the `tfp.mcmc.sample_chain` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the session, get the mean values of the weight and the bias and visualize the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "[A_mcmc, b_mcmc] = sess.run([A, b])\n",
    "\n",
    "print(\"A Coefficient: \", A_mcmc.mean(), '+-', A_mcmc.var()  \n",
    "      , \"\\nb Coefficient: \", b_mcmc.mean(), '+-', b_mcmc.var() )\n",
    "\n",
    "def visualise(X_train, y_train, X_test, y_test, w, b, n_samples=10):\n",
    "    plt.scatter(X_train, y_train)\n",
    "    plt.scatter(X_test, y_test)\n",
    "    inputs = np.linspace(min(X_train.min(), X_test.min()),\n",
    "                         max(X_train.max(), X_test.max()), \n",
    "                         num=400)\n",
    "    for ns in range(n_samples):\n",
    "        #output = inputs * w[ns] + b[ns]\n",
    "        output = inputs * w.mean() + b.mean()\n",
    "        plt.plot(inputs, np.random.normal(output), 'r+')\n",
    "        \n",
    "visualise(X_train, y_train, X_test, y_test, A_mcmc, b_mcmc, n_samples=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variational Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach for solving the inference problem at hand is the Variational inference one. In this approach, the density function is estimated by choosing a distribution we know (eg. Gaussian) and progressively changing its parameters via optimization until it looks like the one we want to compute, the posterior. \n",
    "This “made-up” distribution we are optimizing is called variational distribution. \n",
    "\n",
    "Ex. Derive mathematically the equivalence between choosing the optimal parameters for the variational distribution and maximizing a lower bound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropout "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout (Hinton et al) is a technique used to avoid over-fitting in our model. In essence, dropout technique zeros out neurons randomly according to a Bernoulli distribution. \n",
    "\n",
    "In the context of Bayesian Deep Learning, dropout can be seen as a Gaussian process approximation. In order to get uncertainty estimates from dropout, we just have to apply it both when performing training and prediction steps.\n",
    "\n",
    "Predictive mean and variance can be obtained from the following equations:\n",
    "\n",
    "$$ \\mathbb{E}(y) \\sim \\frac{1}{T} \\sum_{t=0}^{t=T} \\hat{y}_t (x)$$\n",
    "\n",
    "$$ Var(y) \\sim \\tau^{-1} \\mathbb{I}_D + \\frac{1}{T} \\sum_{t=0}^{t=T} \\hat{y}_t (x)^T \\hat{y}_t (x) - \\mathbb{E}(y)^T\\mathbb{E}(y)$$\n",
    "\n",
    "where $\\hat{y}_t$ are the predictions and \n",
    "\n",
    "$$\\tau = \\frac{l^2 p}{2N\\lambda}$$\n",
    "\n",
    "summarizes our Gaussian process precision, with $l$ a prior length-scale that captures our belief over the function frequency, $p$ the probability of the units not being dropped, $N$ the number of points and $\\lambda$ the weight decay parameter.\n",
    "\n",
    "Let's see how to apply all these concepts taken our cosine regression problem (seen in the first classes of the course) as an example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ex: Implement $\\tau$ parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tau(l, p, N, weight_decay):\n",
    "    return l**2 * (1 - p) / (2 * N * weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras import losses\n",
    "from keras import metrics\n",
    "\n",
    "f = lambda x: np.cos(x)\n",
    "x_train = np.linspace(-2*np.pi, 1.5*np.pi,10000)\n",
    "y_train = f(x_train)\n",
    "\n",
    "x_test = np.linspace(1.505*np.pi,2.5*np.pi,50)\n",
    "y_test = f(x_test)\n",
    "\n",
    "x_train_norm = (x_train-x_train.mean())/(x_train.std())\n",
    "y_train_norm = (y_train-y_train.mean())/(y_train.std())\n",
    "\n",
    "plt.plot(x_train, y_train, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "https://arxiv.org/pdf/1601.00670.pdf\n",
    "\n",
    "http://bridg.land/posts/gaussian-processes-1\n",
    "\n",
    "https://arxiv.org/pdf/1703.04977.pdf\n",
    "\n",
    "https://medium.com/tensorflow/introducing-tensorflow-probability-dca4c304e245\n",
    "\n",
    "http://mlg.eng.cam.ac.uk/yarin/blog_3d801aa532c1ce.html\n",
    "\n",
    "https://medium.com/@joeDiHare/deep-bayesian-neural-networks-952763a9537\n",
    "\n",
    "http://pyro.ai/examples/bayesian_regression.html\n",
    "\n",
    "http://edwardlib.org/tutorials/supervised-regression\n",
    "\n",
    "https://docs.pymc.io/notebooks/bayesian_neural_network_advi.html\n",
    "\n",
    "https://github.com/arturzeitler/Bayes-and-MC/blob/master/Bayesian_NN_Example.ipynb\n",
    "\n",
    "https://arxiv.org/pdf/1610.09787.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dl_win_gpu]",
   "language": "python",
   "name": "conda-env-dl_win_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
